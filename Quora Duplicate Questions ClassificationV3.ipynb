{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from utils.clean import text_to_wordlist\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from utils.tweaks import reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 104.87 MB --> 55.91 MB (Decreased by 46.7%)\n",
      "CPU times: user 2.34 s, sys: 325 ms, total: 2.67 s\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('data/train_clean.csv')\n",
    "train = reduce_mem_usage(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>freq_qid1</th>\n",
       "      <th>freq_qid2</th>\n",
       "      <th>q1len</th>\n",
       "      <th>q2len</th>\n",
       "      <th>...</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>token_set_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>wmd</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107084</td>\n",
       "      <td>0.049209</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.093863</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.147856</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what is the story of kohinoor koh - i - noor d...</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085667</td>\n",
       "      <td>0.078207</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.104693</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.554370</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118616</td>\n",
       "      <td>0.050967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.562440</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math 23 ^ 24 math is d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079077</td>\n",
       "      <td>0.053603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.090253</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.184910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120264</td>\n",
       "      <td>0.033392</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.072202</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.795780</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  what is the step by step guide to invest in sh...   \n",
       "1   1     3     4  what is the story of kohinoor koh - i - noor d...   \n",
       "2   2     5     6  how can i increase the speed of my internet co...   \n",
       "3   3     7     8  why am i mentally very lonely how can i solve it    \n",
       "4   4     9    10  which one dissolve in water quikly sugar salt ...   \n",
       "\n",
       "                                           question2  is_duplicate  freq_qid1  \\\n",
       "0  what is the step by step guide to invest in sh...             0   0.000000   \n",
       "1  what would happen if the indian government sto...             0   0.061224   \n",
       "2  how can internet speed be increased by hacking...             0   0.000000   \n",
       "3  find the remainder when math 23 ^ 24 math is d...             0   0.000000   \n",
       "4            which fish would survive in salt water              0   0.040816   \n",
       "\n",
       "   freq_qid2     q1len     q2len  ...  first_word_eq  abs_len_diff  mean_len  \\\n",
       "0        0.0  0.107084  0.049209  ...            1.0      0.008772  0.093863   \n",
       "1        0.0  0.085667  0.078207  ...            1.0      0.021930  0.104693   \n",
       "2        0.0  0.118616  0.050967  ...            1.0      0.017544  0.086643   \n",
       "3        0.0  0.079077  0.053603  ...            0.0      0.013158  0.090253   \n",
       "4        0.0  0.120264  0.033392  ...            1.0      0.026316  0.072202   \n",
       "\n",
       "   longest_substr_ratio  token_set_ratio  token_sort_ratio  fuzz_ratio  \\\n",
       "0              0.982759             1.00              0.93        0.93   \n",
       "1              0.611111             0.86              0.63        0.67   \n",
       "2              0.166667             0.66              0.66        0.54   \n",
       "3              0.040000             0.36              0.36        0.35   \n",
       "4              0.175000             0.67              0.47        0.45   \n",
       "\n",
       "   fuzz_partial_ratio       wmd   jaccard  \n",
       "0                1.00  0.147856  0.916667  \n",
       "1                0.75  0.554370  0.444444  \n",
       "2                0.54  0.562440  0.200000  \n",
       "3                0.39  1.184910  0.000000  \n",
       "4                0.56  0.795780  0.250000  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['freq_qid1',\n",
       " 'freq_qid2',\n",
       " 'q1len',\n",
       " 'q2len',\n",
       " 'q1_n_words',\n",
       " 'q2_n_words',\n",
       " 'word_Common',\n",
       " 'word_Total',\n",
       " 'word_share',\n",
       " 'freq_q1+q2',\n",
       " 'freq_q1-q2',\n",
       " 'cwc_min',\n",
       " 'cwc_max',\n",
       " 'csc_min',\n",
       " 'csc_max',\n",
       " 'ctc_min',\n",
       " 'ctc_max',\n",
       " 'last_word_eq',\n",
       " 'first_word_eq',\n",
       " 'abs_len_diff',\n",
       " 'mean_len',\n",
       " 'longest_substr_ratio',\n",
       " 'token_set_ratio',\n",
       " 'token_sort_ratio',\n",
       " 'fuzz_ratio',\n",
       " 'fuzz_partial_ratio',\n",
       " 'wmd',\n",
       " 'jaccard']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list = train.columns.tolist()[6:]\n",
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[features_list], train['is_duplicate'], test_size=0.33, random_state=42,stratify=train['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.02,\n",
    "    'max_depth': 4,\n",
    "    }\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "model = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(model,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [50, 100]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.1\n",
    "\n",
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)))\n",
    "\n",
    "def create_model():\n",
    "    # Taking the question1 as input and ceating a embedding for each question before feed it to neural network\n",
    "    q1 = layers.Input(shape=(1,), dtype=tf.string)\n",
    "    embedding_q1 = layers.Lambda(UniversalEmbedding, output_shape=(512,))(q1)\n",
    "    # Taking the question2 and doing the same thing mentioned above, using the lambda function\n",
    "    q2 = layers.Input(shape=(1,), dtype=tf.string)\n",
    "    embedding_q2 = layers.Lambda(UniversalEmbedding, output_shape=(512,))(q2)\n",
    "\n",
    "    # Concatenating the both input layer\n",
    "    merged = layers.concatenate([embedding_q1, embedding_q2])\n",
    "    merged = layers.Dense(200, activation='relu')(merged)\n",
    "    merged = layers.Dropout(DROPOUT)(merged)\n",
    "\n",
    "    # Normalizing the input layer,applying dense and dropout  layer for fully connected model and to avoid overfitting \n",
    "    merged = layers.BatchNormalization()(merged)\n",
    "    merged = layers.Dense(200, activation='relu')(merged)\n",
    "    merged = layers.Dropout(DROPOUT)(merged)\n",
    "\n",
    "    merged = layers.BatchNormalization()(merged)\n",
    "    merged = layers.Dense(200, activation='relu')(merged)\n",
    "    merged = layers.Dropout(DROPOUT)(merged)\n",
    "\n",
    "    merged = layers.BatchNormalization()(merged)\n",
    "    merged = layers.Dense(200, activation='relu')(merged)\n",
    "    merged = layers.Dropout(DROPOUT)(merged)\n",
    "\n",
    "    # Using the Sigmoid as the activation function and binary crossentropy for binary classifcation as 0 or 1\n",
    "    merged = layers.BatchNormalization()(merged)\n",
    "    pred = layers.Dense(2, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[q1,q2], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = train['question1']\n",
    "X2 = train['question2']\n",
    "y = train['is_duplicate']\n",
    "# Using the sklearn to split data in question1 and question2 train and test in the ration 80-20 %\n",
    "X1_train, X1_test,X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_q1 = X1_train.tolist()\n",
    "train_q1 = np.array(train_q1, dtype=object)[:, np.newaxis]\n",
    "train_q2 = X2_train.tolist()\n",
    "train_q2 = np.array(train_q2, dtype=object)[:, np.newaxis]\n",
    "\n",
    "train_labels = np.asarray(pd.get_dummies(y_train), dtype = np.int8)\n",
    "\n",
    "test_q1 = X1_test.tolist()\n",
    "test_q1 = np.array(test_q1, dtype=object)[:, np.newaxis]\n",
    "test_q2 = X2_test.tolist()\n",
    "test_q2 = np.array(test_q2, dtype=object)[:, np.newaxis]\n",
    "\n",
    "test_labels = np.asarray(pd.get_dummies(y_test), dtype = np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 512)          0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 512)          0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 200)          205000      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 200)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 200)          800         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 200)          40200       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200)          800         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 200)          40200       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200)          800         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 200)          40200       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200)          800         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            402         batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 329,202\n",
      "Trainable params: 327,602\n",
      "Non-trainable params: 1,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Creating the tensorflow session to train the model and save checkpoint after every epoch.\n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "\n",
    "    filepath=\"models/model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto', period=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    history = nn_model.fit([train_q1, train_q2], \n",
    "            train_labels,\n",
    "            validation_data=([test_q1, test_q2], test_labels),\n",
    "            epochs=10,\n",
    "            batch_size=512, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Question 1 here --> how are you\n",
      "Type Question 2 here --> are you alright\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----FINAL RESULT----\n",
      "****Questions are not Similar****\n"
     ]
    }
   ],
   "source": [
    "q1 = input(\"Type Question 1 here -->\")\n",
    "q2 = input(\"Type Question 2 here -->\") \n",
    "q1 = np.array([[q1],[q1]])\n",
    "q2 = np.array([[q2],[q2]])\n",
    "# Using the same tensorflow session for embedding the test string\n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    # Loading the save weights\n",
    "    nn_model.load_weights('models/model-01-0.50.hdf5')  \n",
    "    # Predicting the similarity between the two input questions \n",
    "    predicts = nn_model.predict([q1, q2], verbose=0)\n",
    "    predict_logits = predicts.argmax(axis=1)\n",
    "    print(\"----FINAL RESULT----\")\n",
    "    if(predict_logits[0] == 1):\n",
    "        print(\"****Questions are Similar****\")\n",
    "    else:\n",
    "        print(\"****Questions are not Similar****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
